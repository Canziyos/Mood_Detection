@article{dyer2016clinical,
  author    = {Dyer, Suzanne M. and Laver, Kate and Pond, Constance D. and Cumming, Robert G. and Whitehead, Craig and Crotty, Maria},
  title     = {Clinical practice guidelines and principles of care for people with dementia in Australia},
  journal   = {Australian Family Physician},
  volume    = {45},
  number    = {12},
  pages     = {884--889},
  year      = {2016},
  month     = {December},
  issn      = {0300-8495},
  pmid      = {27903038},
  url       = {https://pubmed.ncbi.nlm.nih.gov/27903038/},
  note      = {PubMed ID: 27903038}
}

@article{s23052455,
  author    = {Cai, Yujian and Li, Xingguang and Li, Jinsong},
  title     = {Emotion Recognition Using Different Sensors, Emotion Models, Methods and Datasets: A Comprehensive Review},
  journal   = {Sensors},
  volume    = {23},
  number    = {5},
  year      = {2023},
  article-number = {2455},
  doi       = {10.3390/s23052455},
  url       = {https://www.mdpi.com/1424-8220/23/5/2455},
  issn      = {1424-8220},
  note      = {PubMed ID: 36904659}
}

@article{MAJUMDER2018124,
title = {Multimodal sentiment analysis using hierarchical fusion with context modeling},
journal = {Knowledge-Based Systems},
volume = {161},
pages = {124-133},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.07.041},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118303897},
author = {N. Majumder and D. Hazarika and A. Gelbukh and E. Cambria and S. Poria},
keywords = {Multimodal fusion, Sentiment analysis},
abstract = {Multimodal sentiment analysis is a very actively growing field of research. A promising area of opportunity in this field is to improve the multimodal fusion mechanism. We present a novel feature fusion strategy that proceeds in a hierarchical fashion, first fusing the modalities two in two and only then fusing all three modalities. On multimodal sentiment analysis of individual utterances, our strategy outperforms conventional concatenation of features by 1%, which amounts to 5% reduction in error rate. On utterance-level multimodal sentiment analysis of multi-utterance video clips, for which current state-of-the-art techniques incorporate contextual information from other utterances of the same clip, our hierarchical fusion gives up to 2.4% (almost 10% error rate reduction) over currently used concatenation. The implementation of our method is publicly available in the form of open-source code.}
}

@ARTICLE{8952721,
  author={Du, Guanglong and Long, Shuaiying and Yuan, Hua},
  journal={IEEE Access}, 
  title={Non-Contact Emotion Recognition Combining Heart Rate and Facial Expression for Interactive Gaming Environments}, 
  year={2020},
  volume={8},
  number={},
  pages={11896-11906},
  keywords={Heart rate;Emotion recognition;Games;Feature extraction;Iron;Speech recognition;Physiology;Contactless emotion recognition;facial expression;heart rate;game evaluation},
  doi={10.1109/ACCESS.2020.2964794}
}



@Article{s23125475,
AUTHOR = {Mamieva, Dilnoza and Abdusalomov, Akmalbek Bobomirzaevich and Kutlimuratov, Alpamis and Muminov, Bahodir and Whangbo, Taeg Keun},
TITLE = {Multimodal Emotion Detection via Attention-Based Fusion of Extracted Facial and Speech Features},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {12},
ARTICLE-NUMBER = {5475},
URL = {https://www.mdpi.com/1424-8220/23/12/5475},
PubMedID = {37420642},
ISSN = {1424-8220},
ABSTRACT = {Methods for detecting emotions that employ many modalities at the same time have been found to be more accurate and resilient than those that rely on a single sense. This is due to the fact that sentiments may be conveyed in a wide range of modalities, each of which offers a different and complementary window into the thoughts and emotions of the speaker. In this way, a more complete picture of a person’s emotional state may emerge through the fusion and analysis of data from several modalities. The research suggests a new attention-based approach to multimodal emotion recognition. This technique integrates facial and speech features that have been extracted by independent encoders in order to pick the aspects that are the most informative. It increases the system’s accuracy by processing speech and facial features of various sizes and focuses on the most useful bits of input. A more comprehensive representation of facial expressions is extracted by the use of both low- and high-level facial features. These modalities are combined using a fusion network to create a multimodal feature vector which is then fed to a classification layer for emotion recognition. The developed system is evaluated on two datasets, IEMOCAP and CMU-MOSEI, and shows superior performance compared to existing models, achieving a weighted accuracy WA of 74.6% and an F1 score of 66.1% on the IEMOCAP dataset and a WA of 80.7% and F1 score of 73.7% on the CMU-MOSEI dataset.},
DOI = {10.3390/s23125475}
}

@Article{s20010183,
AUTHOR = {Mustaqeem and Kwon, Soonil},
TITLE = {A CNN-Assisted Enhanced Audio Signal Processing for Speech Emotion Recognition},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {183},
URL = {https://www.mdpi.com/1424-8220/20/1/183},
PubMedID = {31905692},
ISSN = {1424-8220},
ABSTRACT = {Speech is the most significant mode of communication among human beings and a potential method for human-computer interaction (HCI) by using a microphone sensor. Quantifiable emotion recognition using these sensors from speech signals is an emerging area of research in HCI, which applies to multiple applications such as human-reboot interaction, virtual reality, behavior assessment, healthcare, and emergency call centers to determine the speaker’s emotional state from an individual’s speech. In this paper, we present major contributions for; (i) increasing the accuracy of speech emotion recognition (SER) compared to state of the art and (ii) reducing the computational complexity of the presented SER model. We propose an artificial intelligence-assisted deep stride convolutional neural network (DSCNN) architecture using the plain nets strategy to learn salient and discriminative features from spectrogram of speech signals that are enhanced in prior steps to perform better. Local hidden patterns are learned in convolutional layers with special strides to down-sample the feature maps rather than pooling layer and global discriminative features are learned in fully connected layers. A SoftMax classifier is used for the classification of emotions in speech. The proposed technique is evaluated on Interactive Emotional Dyadic Motion Capture (IEMOCAP) and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) datasets to improve accuracy by 7.85% and 4.5%, respectively, with the model size reduced by 34.5 MB. It proves the effectiveness and significance of the proposed SER technique and reveals its applicability in real-world applications.},
DOI = {10.3390/s20010183}
}

@ARTICLE{9521479,
  author={Zhang, Li and Fu, Chang-Hong and Hong, Hong and Xue, Biao and Gu, Xuemei and Zhu, Xiaohua and Li, Changzhi},
  journal={IEEE Sensors Journal}, 
  title={Non-Contact Dual-Modality Emotion Recognition System by CW Radar and RGB Camera}, 
  year={2021},
  volume={21},
  number={20},
  pages={23198-23212},
  keywords={Biomedical monitoring;Heart beat;Sensors;Monitoring;Doppler radar;Emotion recognition;Cameras;Non-contact;dual-modality;radar;video;emotion recognition},
  doi={10.1109/JSEN.2021.3107429}
}


@ARTICLE{10026861,
  author={Pan, Jiahui and Fang, Weijie and Zhang, Zhihang and Chen, Bingzhi and Zhang, Zheng and Wang, Shuihua},
  journal={IEEE Open Journal of Engineering in Medicine and Biology}, 
  title={Multimodal Emotion Recognition Based on Facial Expressions, Speech, and EEG}, 
  year={2024},
  volume={5},
  number={},
  pages={396-403},
  keywords={Emotion recognition;Brain modeling;Feature extraction;Electroencephalography;Speech recognition;Convolution;Deep learning;Multimodal emotion recognition;electroencephalogram;facial expressions;speech},
  doi={10.1109/OJEMB.2023.3240280}
}

@article{https://doi.org/10.1155/2017/2107451,
author = {Huang, Yongrui and Yang, Jianhao and Liao, Pengkai and Pan, Jiahui},
title = {Fusion of Facial Expressions and EEG for Multimodal Emotion Recognition},
journal = {Computational Intelligence and Neuroscience},
volume = {2017},
number = {1},
pages = {2107451},
doi = {https://doi.org/10.1155/2017/2107451},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2017/2107451},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2017/2107451},
abstract = {This paper proposes two multimodal fusion methods between brain and peripheral signals for emotion recognition. The input signals are electroencephalogram and facial expression. The stimuli are based on a subset of movie clips that correspond to four specific areas of valance-arousal emotional space (happiness, neutral, sadness, and fear). For facial expression detection, four basic emotion states (happiness, neutral, sadness, and fear) are detected by a neural network classifier. For EEG detection, four basic emotion states and three emotion intensity levels (strong, ordinary, and weak) are detected by two support vector machines (SVM) classifiers, respectively. Emotion recognition is based on two decision-level fusion methods of both EEG and facial expression detections by using a sum rule or a production rule. Twenty healthy subjects attended two experiments. The results show that the accuracies of two multimodal fusion detections are 81.25\% and 82.75\%, respectively, which are both higher than that of facial expression (74.38\%) or EEG detection (66.88\%). The combination of facial expressions and EEG information for emotion recognition compensates for their defects as single information sources.},
year = {2017}
}

@INPROCEEDINGS{10446459,
  author={Zhang, Chengwen and Zhang, Yuhao and Cheng, Bo},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={RL-EMO: A Reinforcement Learning Framework for Multimodal Emotion Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={10246-10250},
  keywords={Emotion recognition;Codes;Convolution;Semantics;Reinforcement learning;Speech recognition;Acoustics;Reinforcement Learning;Multimodal Emotion Recognition},
  doi={10.1109/ICASSP48485.2024.10446459}
  }


@Article{app14178071,
  AUTHOR = {Udahemuka, Gustave and Djouani, Karim and Kurien, Anish M.},
  TITLE = {Multimodal Emotion Recognition Using Visual, Vocal and Physiological Signals: A Review},
  JOURNAL = {Applied Sciences},
  VOLUME = {14},
  YEAR = {2024},
  NUMBER = {17},
  ARTICLE-NUMBER = {8071},
  URL = {https://www.mdpi.com/2076-3417/14/17/8071},
  ISSN = {2076-3417},
  ABSTRACT = {The dynamic expressions of emotion convey both the emotional and functional states of an individual’s interactions. Recognizing the emotional states helps us understand human feelings and thoughts. Systems and frameworks designed to recognize human emotional states automatically can use various affective signals as inputs, such as visual, vocal and physiological signals. However, emotion recognition via a single modality can be affected by various sources of noise that are specific to that modality and the fact that different emotion states may be indistinguishable. This review examines the current state of multimodal emotion recognition methods that integrate visual, vocal or physiological modalities for practical emotion computing. Recent empirical evidence on deep learning methods used for fine-grained recognition is reviewed, with discussions on the robustness issues of such methods. This review elaborates on the profound learning challenges and solutions required for a high-quality emotion recognition system, emphasizing the benefits of dynamic expression analysis, which aids in detecting subtle micro-expressions, and the importance of multimodal fusion for improving emotion recognition accuracy. The literature was comprehensively searched via databases with records covering the topic of affective computing, followed by rigorous screening and selection of relevant studies. The results show that the effectiveness of current multimodal emotion recognition methods is affected by the limited availability of training data, insufficient context awareness, and challenges posed by real-world cases of noisy or missing modalities. The findings suggest that improving emotion recognition requires better representation of input data, refined feature extraction, and optimized aggregation of modalities within a multimodal framework, along with incorporating state-of-the-art methods for recognizing dynamic expressions.},
  DOI = {10.3390/app14178071}
  }

@article{WOZNIAK20232556,
title = {Bimodal Emotion Recognition Based on Vocal and Facial Features},
journal = {Procedia Computer Science},
volume = {225},
pages = {2556-2566},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.247},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923014059},
author = {Mateusz Wozniak and Michal Sakowicz and Kacper Ledwosinski and Jakub Rzepkowski and Pawel Czapla and Szymon Zaporowski},
keywords = {Emotion recognition, TFusion, Deep Learning, Affective Computing},
abstract = {Emotion recognition is a crucial aspect of human communication, with applications in fields such as psychology, education, and healthcare. Identifying emotions accurately is challenging, as people use a variety of signals to express and perceive emotions. In this study, we address the problem of multimodal emotion recognition using both audio and video signals, to develop a robust and reliable system that can recognize emotions even when one modality is absent. To achieve this goal, we propose a novel architecture based on well-designed feature extractors for each modality and use model-level fusion based on a TFusion block to combine the information from both sources. To be more efficient in real-world scenarios, we trained our model on a compound dataset consisting of RAVDESS, RML, and eNTERFACE'05. It is then evaluated and compared to the state-of-the-art models. We find that our approach performs close to the modern solutions and can recognize emotions accurately when one of the modalities is missing. Additionally, we have developed a real-time emotion recognition application as a part of this work.}
}




